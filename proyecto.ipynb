{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import os\n",
    "from os.path import isfile, join, isdir\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listando directorios de libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de libros: 7\n"
     ]
    }
   ],
   "source": [
    "dir_path = 'data'\n",
    "#os.listdir(dir_path)\n",
    "listdir = [dir_path + '/' + d for d in os.listdir(dir_path) if isdir(join(dir_path, d))]\n",
    "listdir.sort()\n",
    "\n",
    "print ('Cantidad de libros: ' + str(len(listdir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_tag = ['p', 'p', 'div', 'p', 'p', 'p', 'p' ]\n",
    "list_class = [['para-center', 'para-flush','para-indent', 'para-quote', 'para-ragged-left', 'para-ragged-right', 'sans-para-center', 'sans-para-flush', 'sans-para-indent', ''],\n",
    "              ['para-flush','para-indent'],\n",
    "              ['tx','tx1'], \n",
    "              ['begin', 'para-indent', 'para-flush','para-ragged-left' ],\n",
    "              ['block','blockb','blocki','indent','indent1','indent1b','indenta', 'indentab','indentb','indenti', 'noindent','noindenta','noindentb'],\n",
    "              ['block','blocki','blockt','blockti','indent', 'indenta', 'indentb', 'noindent', 'noindent1', 'noindenta', 'noindentb', 'poem', 'poem1', 'poema', 'right', 'right1'],\n",
    "              ['indent', 'indentb', 'indentb1', 'noindent', 'noindentc', 'noindentn', 'noindentn1']\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo el texto de cada libro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validar_parrafo_no_tag(parrafo):\n",
    "    tags = ['<', '>']\n",
    "    result = [tag in parrafo for tag in tags]\n",
    "    final = any(result)\n",
    "    return not final\n",
    "\n",
    "def print_text_if_class(parrafo, html_cl):\n",
    "    if(html_cl in parrafo['class']):\n",
    "        print(''.join(parrafo.findAll(text=True)))\n",
    "\n",
    "def validar_parrafo_clases(parrafo, clases):\n",
    "    l_valido = [c in parrafo['class'] for c in clases]\n",
    "    return any(l_valido)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leer_parrafos(soupObj, html_tag, html_clases):\n",
    "    parrafos = soupObj.find_all(html_tag)\n",
    "    parrafos_validos = [validar_parrafo_clases(p, html_clases) for p in parrafos]\n",
    "    parrafos_final = list(compress(parrafos, parrafos_validos))\n",
    "    parrafos_str = [unicode(''.join(p.findAll(text=True))) \\\n",
    "                    for p in parrafos_final \\\n",
    "                    if type(p.findAll(text=True)[0]) == bs4.element.NavigableString]\n",
    "    return parrafos_str\n",
    "\n",
    "def leer_parrafos_bs4(soupObj, html_tag):\n",
    "    parrafos = soupObj.find_all(html_tag)\n",
    "    return parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leer_libro(book_path, html_tag, html_clases):\n",
    "    list_files = [book_path + '/' + f for f in os.listdir(book_path) if isfile(join(book_path, f))]\n",
    "    list_files.sort()\n",
    "    print ('Cantidad de archivos en <' + book_path + '>: ' + str(len(list_files)))\n",
    "    \n",
    "    list_html = [open(f,'r') for f in list_files]\n",
    "    l_BsOj = [ BeautifulSoup(html, 'html5lib') for html in list_html]\n",
    "    \n",
    "    parrafos_matriz = [leer_parrafos(soupObj, html_tag, html_clases) for soupObj in l_BsOj]\n",
    "    parrafos_array = [unidecode(p) for p_list in parrafos_matriz for p in p_list]\n",
    "    \n",
    "    l_vf = [validar_parrafo_no_tag(p) for p in parrafos_array]\n",
    "    verificar = all(l_vf)\n",
    "    total = len(parrafos_array)\n",
    "    \n",
    "    print('Parrafos validos? ' + str(verificar))\n",
    "    print('Cantidad total de parrafos: ' + str(total))\n",
    "    return parrafos_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag html a analizar: <p>\n",
      "Clases html a analizar: para-center, para-flush, para-indent, para-quote, para-ragged-left, para-ragged-right, sans-para-center, sans-para-flush, sans-para-indent, \n",
      "Cantidad de archivos en <data/book1>: 5\n",
      "Parrafos validos? True\n",
      "Cantidad total de parrafos: 2046\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "book1 = listdir[0]\n",
    "l_par = []\n",
    "for book, html_tag, html_clases in zip(listdir, list_tag, list_class):\n",
    "    print('Tag html a analizar: <' + html_tag + '>')\n",
    "    print('Clases html a analizar: ' + ', '.join(html_clases))\n",
    "    parrafos = leer_libro(book, html_tag, html_clases)\n",
    "    l_par.append(parrafos)\n",
    "    print\n",
    "    print\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from corenlp import *\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('../stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '../stanford-ner/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jsonrpc import ServerProxy, JsonRpc20, TransportTcpIp\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__ (self):\n",
    "        self.server = ServerProxy(JsonRpc20(),\n",
    "                                 TransportTcpIp(addr=(\"127.0.0.1\", 8080)))\n",
    "    def parse (self, text):\n",
    "        return json.loads(self.server.parse(text))\n",
    "    \n",
    "nlp = StanfordNLP()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alias = {\n",
    "    'Roland': ['the gunslinger'],\n",
    "    'Eddie': ['a drog addict', 'the prisioner'],\n",
    "    'Jake': ['Bama', 'the boy'],\n",
    "    'Oy': ['billy-bumbler', 'oy'],\n",
    "    'Cuthbert': ['Bert'], \n",
    "    'John':  ['the good man'],\n",
    "    'Randal': ['the man in black', 'the ageless stranger', 'the walking dude',\\\n",
    "              'walter o\\'dim','walter o`dim', 'marten broadcloak', 'richard faninn', \\\n",
    "             'rudin filaro', 'legion', 'covenant man', 'walter padick', 'son of sam'],\n",
    "    'Aballah': ['the crimson king', 'the ultimate evil', 'ram aballah'],\n",
    "    'Jack': ['the pusher'],\n",
    "    'Blaine': ['blaine the mono'],\n",
    "    'Rhea': ['rhea of the cöos'],\n",
    "    'Andrew': ['the tick-tock man']\n",
    "}\n",
    "\n",
    "global_entidades = set(['Roland', 'Eddie', 'Jake', 'Oy',\n",
    "              'Cuthbert', 'John', 'Randal', 'Aballah',\n",
    "              'Jack', 'Blaine', 'Rhea', 'Andrew'])\n",
    "\n",
    "coincidencias = {\n",
    "    'Roland': 0, 'Eddie': 0, 'Jake': 0, 'Oy': 0, 'Cuthbert': 0,\n",
    "    'John': 0, 'Randal': 0, 'Aballah': 0, 'Jack': 0,\n",
    "    'Blaine': 0, 'Rhea': 0, 'Andrew': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def revisar_parrafo_tok(p, alias):\n",
    "    p_tok = word_tokenize(p.decode('utf-8') , language='english')\n",
    "    for word in p:\n",
    "        if (word == 'oy'):\n",
    "            print p\n",
    "        if (alias == word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def cambiar_alias(parrafo, alias, coincidencias):\n",
    "    new_p = parrafo\n",
    "    for entidad, alias in alias.iteritems():\n",
    "        for to_replace in alias:\n",
    "            #to low\n",
    "            p_low = new_p.lower()\n",
    "            #buscar substr\n",
    "            coincide = True\n",
    "            if (len(to_replace.split(' ')) == 1):\n",
    "                coincide = revisar_parrafo_tok(p_low, to_replace)\n",
    "            ind_st = p_low.find(to_replace)\n",
    "            if (ind_st < 0 or not coincide): continue\n",
    "            ind_lt = ind_st + len(to_replace)\n",
    "            #remplazar por entidad\n",
    "            new_p = new_p[:ind_st] + entidad + new_p[ind_lt:]\n",
    "            #aumentar contador de apariencias\n",
    "            coincidencias[entidad] += 1\n",
    "            if (to_replace == 'the mono'):\n",
    "                print(parrafo)\n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_book_par = []\n",
    "\n",
    "for book in l_par:\n",
    "    new_b = [cambiar_alias(p, alias, coincidencias) for p in book]\n",
    "    new_book_par.append(new_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenar_parrafos_cortos(book, min_sent):\n",
    "    new_book = []\n",
    "    book_len = len(book)\n",
    "    bef_short = False\n",
    "    for p in book:\n",
    "        if bef_short:\n",
    "            last_char = new_book[-1][-1]\n",
    "            if (last_char == '.' or last_char == '!' or last_char == '?' or last_char=='\"'):\n",
    "                new_book[-1] = new_book[-1] + ' ' + p  \n",
    "            else:\n",
    "                new_book[-1] = new_book[-1] + '. ' + p\n",
    "        else:\n",
    "            new_book.append(p)\n",
    "        bef_short = len(sent_tokenize(p.decode('utf-8'))) <= min_sent\n",
    "    return new_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minima sentencias en un parrafo\n",
    "min_sent = 3\n",
    "\n",
    "newest_book_par = [concatenar_parrafos_cortos(b, min_sent) for b in new_book_par]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aballah': 1,\n",
       " 'Andrew': 0,\n",
       " 'Blaine': 0,\n",
       " 'Cuthbert': 0,\n",
       " 'Eddie': 0,\n",
       " 'Jack': 0,\n",
       " 'Jake': 250,\n",
       " 'John': 6,\n",
       " 'Oy': 0,\n",
       " 'Randal': 143,\n",
       " 'Rhea': 0,\n",
       " 'Roland': 486}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#para parrafos que contengan m'as de una oraci'on\n",
    "def reemplazar_referencias_parrafo(text, nlp):\n",
    "    print 'Original text:'\n",
    "    print(text)\n",
    "    #nlp = StanfordCoreNLP()\n",
    "    result = nlp.parse(text)\n",
    "    tokenized_sentences = nltk.sent_tokenize(text)\n",
    "    if (len(tokenized_sentences) == 1) :\n",
    "        print \" >  Solo hay una oracion, no existen relaciones.\"\n",
    "        return\n",
    "    \n",
    "    tokenized_in_words=[nltk.word_tokenize(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "    for block_to_replace in result[\"coref\"]:\n",
    "        sentence_index=block_to_replace[0][1][1]\n",
    "        word_index=block_to_replace[0][1][2]\n",
    "        replace_sent=block_to_replace[0][1][0]\n",
    "        word_to_replace=tokenized_in_words[sentence_index][word_index]\n",
    "\n",
    "        if not word_to_replace in personajes:\n",
    "            continue\n",
    "\n",
    "\n",
    "        print 'Word_to_replace: ',word_to_replace\n",
    "\n",
    "        for i,lines_to_replace in enumerate(block_to_replace):\n",
    "            print \"Converting sentence number \",i\n",
    "\n",
    "            ix_sent=lines_to_replace[0][1]\n",
    "            sent_to_replace=lines_to_replace[0][0]\n",
    "            tokenized_sentences[ix_sent]=tokenized_sentences[ix_sent].replace(sent_to_replace,word_to_replace)\n",
    "\n",
    "        tokenized_sentences[sentence_index]=tokenized_sentences[sentence_index].replace(replace_sent,word_to_replace)\n",
    "        print\n",
    "\n",
    "    final_text=' '.join(tokenized_sentences)\n",
    "    return final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "He did not take the flint and steel from his purse until the remains of the day were only fugitive heat in the ground beneath him and a sardonic orange line on the monochrome horizon. He sat with his gunna drawn across his lap and watched the southeast patiently, looking toward the mountains, not hoping to see the thin straight line of smoke from a new campfire, not expecting to see an orange spark of flame, but watching anyway because watching was a part of it, and had its own bitter satisfaction. You will not see what you do not look for, maggot, Cort would have said. Open the gobs the gods gave ya, will ya not?\n",
      "Word_to_replace:  He\n",
      "Converting sentence number  0\n",
      "Converting sentence number  1\n",
      "Converting sentence number  2\n",
      "Converting sentence number  3\n",
      "Converting sentence number  4\n",
      "\n",
      "Word_to_replace:  He\n",
      "Converting sentence number  0\n",
      "\n",
      "He did not take the flint and steel from He purse until the remains of the day were only fugitive heat in the ground beneath He and a sardonic orange line on the monochrome horizon. He sat wHeh He gunna drawn across He lap and watched the southeast patiently, looking toward the mountains, not hoping to see the thin straight line of smoke from a new campfire, not expecting to see He spark of flame, but watching anyway because watching was a part of He, and had Hes own bHeter satisfaction. You will not see what you do not look for, maggot, Cort would have said. Open the gobs the gods gave ya, will ya not?\n"
     ]
    }
   ],
   "source": [
    "print reemplazar_referencias_parrafo(newest_book_par[0][10] , nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizar_parrafo(text):\n",
    "    tokenized_sentences = nltk.sent_tokenize(text)\n",
    "    sents_token=[nltk.word_tokenize(sentence) for sentence in tokenized_sentences]\n",
    "    return sents_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unificador_nnp_extractor_verbos(sents_token):\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in sents_token]\n",
    "    ner_sentences = [st.tag(sentence) for sentence in sents_token]\n",
    "    paragraph_untokenized=[[] for s in sents_token]\n",
    "    verbs=[[] for s in sents_token]\n",
    "    entities=[{\"PERSON\":[],\"ORGANIZATION\":[],\"LOCATION\":[]} for s in sents_token]\n",
    "    nnp_before = False\n",
    "    for sentenceNer,sentencePos, i in zip(ner_sentences,tagged_sentences,range(len(ner_sentences))):\n",
    "        flag_aux = False\n",
    "        flag_ver = False\n",
    "        flag_final = False\n",
    "        polarity = False\n",
    "        for j,(wordNer,tagNer),(wordPos,tagPos) in zip(range(len(sentenceNer)),sentenceNer,sentencePos):\n",
    "            if j > 0 and tagNer != 'O' :\n",
    "                if nnp_before:\n",
    "                    paragraph_untokenized[i][-1] +=  wordNer\n",
    "                    entities[i][tagNer][-1] += wordNer\n",
    "                else:\n",
    "                    paragraph_untokenized[i].append(wordNer)\n",
    "                    entities[i][tagNer].append(wordNer)\n",
    "                nnp_before=True\n",
    "            else:\n",
    "                nnp_before = False\n",
    "                paragraph_untokenized[i].append(wordNer)\n",
    "                \n",
    "            polarity = flag_final\n",
    "            if flag_final:\n",
    "                verbs[i].pop()\n",
    "            \n",
    "            flag_aux = ('RB' == tagPos) and (wordPos == \"n't\" or wordPos == \"not\")\n",
    "                \n",
    "            flag_final = flag_aux and flag_ver\n",
    "                \n",
    "            if 'VB' in tagPos:\n",
    "                verbs[i].append((wordPos, polarity))\n",
    "                flag_ver = True\n",
    "            else:\n",
    "                flag_ver = False\n",
    "                \n",
    "    #Llenar entidades globales\n",
    "    for e in entities:\n",
    "        for o in e['PERSON']:\n",
    "            global_entidades.add(o)\n",
    "        for o in e['ORGANIZATION']:\n",
    "            global_entidades.add(o)\n",
    "        for o in e['LOCATION']:\n",
    "            global_entidades.add(o);\n",
    "    \n",
    "    return verbs,entities\n",
    "    \n",
    "    \n",
    "    #return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraer_dato_parrafo(parrafo):\n",
    "    token_par=tokenizar_parrafo(parrafo)\n",
    "    verbos,entidades = unificador_nnp_extractor_verbos(token_par)\n",
    "    new_p = []\n",
    "    for v, e in zip(verbos, entidades):\n",
    "        new_p.append({\n",
    "            'verbos': v,\n",
    "            'entidades': e\n",
    "        })\n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entidades': {'LOCATION': [], 'ORGANIZATION': [], 'PERSON': [u'Roland']},\n",
       "  'verbos': [('fled', False), ('followed', False)]},\n",
       " {'entidades': {'LOCATION': [], 'ORGANIZATION': [], 'PERSON': []},\n",
       "  'verbos': [('was', False), ('standing', False), ('looked', False)]},\n",
       " {'entidades': {'LOCATION': [], 'ORGANIZATION': [], 'PERSON': []},\n",
       "  'verbos': [('was', False), ('sketched', False), ('brought', False)]},\n",
       " {'entidades': {'LOCATION': [], 'ORGANIZATION': [], 'PERSON': []},\n",
       "  'verbos': [('pointed', False),\n",
       "   ('cut', False),\n",
       "   ('had', False),\n",
       "   ('been', False)]},\n",
       " {'entidades': {'LOCATION': [], 'ORGANIZATION': [], 'PERSON': []},\n",
       "  'verbos': [('had', False), ('followed', False)]},\n",
       " {'entidades': {'LOCATION': [], 'ORGANIZATION': [], 'PERSON': []},\n",
       "  'verbos': [('had', False), ('moved', False)]},\n",
       " {'entidades': {'LOCATION': [], 'ORGANIZATION': [], 'PERSON': []},\n",
       "  'verbos': [('had', False), ('emptied', False)]}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraer_dato_parrafo(newest_book_par[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aballah',\n",
       " 'Andrew',\n",
       " 'Blaine',\n",
       " 'Cuthbert',\n",
       " 'Eddie',\n",
       " 'Jack',\n",
       " 'Jake',\n",
       " 'John',\n",
       " 'Oy',\n",
       " 'Randal',\n",
       " 'Rhea',\n",
       " 'Roland'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def senti_verb(verb):\n",
    "    list_meanings=swn.senti_synsets(verb[0])\n",
    "    pos_value=0\n",
    "    neg_value=0\n",
    "    n_vals=len(list_meanings)\n",
    "    meaning_counter=0\n",
    "    for meaning in list_meanings:\n",
    "        lit_string=str(meaning)\n",
    "        if not '.v.' in lit_string:\n",
    "            continue\n",
    "        pos_value+=meaning.pos_score()\n",
    "        neg_value+=meaning.neg_score()\n",
    "        meaning_counter+=1\n",
    "        if meaning_counter == 3: break\n",
    "    if n_vals>0:\n",
    "        pos_value/=n_vals\n",
    "        neg_value/=n_vals\n",
    "    if (verb[1]):\n",
    "        return neg_value,pos_value\n",
    "    else:\n",
    "        return pos_value,neg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('take', True)\n",
      "(0.0, 0.002840909090909091)\n"
     ]
    }
   ],
   "source": [
    "print verbos[0][0]\n",
    "print senti_verb(verbos[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
